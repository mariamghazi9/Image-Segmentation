{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport math\nimport csv\nfrom sklearn.metrics.pairwise import euclidean_distances\nfrom sklearn.cluster import spectral_clustering\nfrom sklearn.neighbors import kneighbors_graph\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n        \nGROUND_TRUTH = '/kaggle/input/bsr-images/data-2/data/groundTruth/test'\nRAW_DATA = '/kaggle/input/bsr-images/data-2/data/images/test'\nOUT_CLUSTERING = '/kaggle/working'\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-01T09:06:32.670025Z","iopub.execute_input":"2022-04-01T09:06:32.670708Z","iopub.status.idle":"2022-04-01T09:06:34.132478Z","shell.execute_reply.started":"2022-04-01T09:06:32.670558Z","shell.execute_reply":"2022-04-01T09:06:34.131669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Dataset","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nimport scipy.io as sio\nfrom matplotlib import cm, pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-04-01T09:06:34.134386Z","iopub.execute_input":"2022-04-01T09:06:34.134899Z","iopub.status.idle":"2022-04-01T09:06:34.176445Z","shell.execute_reply.started":"2022-04-01T09:06:34.134851Z","shell.execute_reply":"2022-04-01T09:06:34.175556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"means=[3,5,7,9,11] \n\nselected_images = [] # the IDs of the images to process\nfor _,_,filenames in os.walk(RAW_DATA):\n    for filename in filenames:\n        if '.jpg' in filename:\n            selected_images.append(int(filename[:-4]))\nselected_images = sorted(selected_images)[:50]\nprint(selected_images)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T09:06:34.177748Z","iopub.execute_input":"2022-04-01T09:06:34.178805Z","iopub.status.idle":"2022-04-01T09:06:34.243502Z","shell.execute_reply.started":"2022-04-01T09:06:34.178762Z","shell.execute_reply":"2022-04-01T09:06:34.242462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 100007, 100039 (321, 481, 3)\noriginal_dim = [] # original dimensions of each image\n\ndef load_image(s, disp=True):\n    ground_truths = [] # each element is an image, where each pixel has a label to identify its partition\n    data_to_segment = [] # each element is an image represented as a flattened numpy array of pixels (i.e. 154401 pixels)\n    i_subplot = 0\n    \n    d = os.path.join(OUT_CLUSTERING, str(s))\n    os.makedirs(d, exist_ok=True)\n    \n    # image itself\n    print('Reading image {}'.format(s))\n    im = Image.open(os.path.join(RAW_DATA, '{}.jpg'.format(s)))\n    mat = np.asarray(im)\n    #print(mat.shape)\n    #assert mat.shape == (321, 481, 3) Some images are 481 * 321!!!!!!!!!\n    assert mat.shape[0] * mat.shape[1] == 154401\n    original_dim.append((mat.shape[0], mat.shape[1]))\n    \n    #print(mat)\n    data_to_segment = mat.reshape(154401, 3)\n    \n    # ground truth\n    truths = sio.loadmat(os.path.join(GROUND_TRUTH, '{}.mat'.format(s)))['groundTruth'].squeeze()\n    if disp:\n        # set up figure \n        fig, axs = plt.subplots(1,1 + truths.shape[0], num=1, clear=True)\n        fig.set_size_inches(25,15)\n        axs[i_subplot].axis('off')\n        axs[i_subplot].imshow(im)\n    im.close()\n    i_subplot += 1\n    \n    temp = []\n    for truth in truths:\n        uncovered = truth[0][0][0] # Are they afraid it'll run away? We need the segmentations not the boundaries anyway\n        #assert uncovered.shape == (321, 481)\n        assert uncovered.shape[0] * uncovered.shape[1] == 154401\n        #print(uncovered)\n        temp.append(np.array(uncovered).ravel())\n        uncovered = uncovered /np.max(uncovered)\n        #display(Image.fromarray(cm.gist_earth(uncovered, bytes=True)))\n        if disp:\n            axs[i_subplot].axis('off')\n            #print(uncovered)\n            axs[i_subplot].imshow(uncovered, cmap='gist_earth')\n        i_subplot += 1\n        \n    if disp:\n        fig.savefig(os.path.join(d, 'original_{}.png'.format(s)))\n        \n    #print(temp)\n    ground_truths = temp\n    return data_to_segment, ground_truths","metadata":{"execution":{"iopub.status.busy":"2022-04-01T09:06:34.245897Z","iopub.execute_input":"2022-04-01T09:06:34.246322Z","iopub.status.idle":"2022-04-01T09:06:34.261283Z","shell.execute_reply.started":"2022-04-01T09:06:34.246278Z","shell.execute_reply":"2022-04-01T09:06:34.260402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef display_output(idx, clusterings, method_name):\n    image_name = selected_images[idx]\n    d = os.path.join(OUT_CLUSTERING, str(image_name))\n    \n    mat_dict = {}\n    \n    fig, axs = plt.subplots(1,len(clusterings), clear=True)\n    fig.set_size_inches(25,15)\n    \n    if len(clusterings) == 1:\n        for (i, mat) in enumerate(clusterings):\n            temp = np.array(mat).reshape(original_dim[idx])\n            mat_dict[str(means[i])] = np.copy(temp)\n            temp = temp/np.max(temp)\n            #print(temp)\n            axs.axis('off')\n            axs.imshow(temp, cmap='gist_earth')\n            #display(Image.fromarray(cm.gist_earth(temp, bytes=True)))\n    else:\n        for (i, mat) in enumerate(clusterings):\n            temp = np.array(mat).reshape(original_dim[idx])\n            mat_dict[str(means[i])] = np.copy(temp)\n            temp = temp/np.max(temp)\n            #print(temp)\n            axs[i].axis('off')\n            axs[i].imshow(temp, cmap='gist_earth')\n            #display(Image.fromarray(cm.gist_earth(temp, bytes=True)))\n\n    sio.savemat(os.path.join(d, str(s) + '.mat'), mat_dict)\n    fig.savefig(os.path.join(d,\"{}_{}.png\".format(image_name, method_name)))","metadata":{"execution":{"iopub.status.busy":"2022-04-01T09:06:34.26245Z","iopub.execute_input":"2022-04-01T09:06:34.263384Z","iopub.status.idle":"2022-04-01T09:06:34.280622Z","shell.execute_reply.started":"2022-04-01T09:06:34.263343Z","shell.execute_reply":"2022-04-01T09:06:34.279793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Kmeans","metadata":{}},{"cell_type":"code","source":"def diff(x, y):\n    return abs(x-y)/max(x,y)\n\ndef k_means(image, k, threshold, max_iterations):\n\n    np.random.seed(1)\n    centroids = np.random.randn(k, image.shape[1])\n    prev_loss = 1e9\n    for i in range(max_iterations):\n        dists_to_centroids = euclidean_distances(X=image, Y=centroids)\n        assignment = np.argmin(dists_to_centroids, axis=1)\n        loss = np.sum(np.linalg.norm(centroids[assignment] - image, axis=1)**2)\n        if diff(loss, prev_loss) <= threshold:\n            break\n        prev_loss = loss\n\n        for centroid_index in range(k):\n            cent = image[np.where(assignment==centroid_index)]\n            if cent.shape[0] > 0: \n                centroids[centroid_index] = np.mean(cent, axis=0)\n        \n    return assignment, centroids\n","metadata":{"execution":{"iopub.status.busy":"2022-04-01T09:06:34.281861Z","iopub.execute_input":"2022-04-01T09:06:34.282779Z","iopub.status.idle":"2022-04-01T09:06:34.299204Z","shell.execute_reply.started":"2022-04-01T09:06:34.282733Z","shell.execute_reply":"2022-04-01T09:06:34.298295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cluster, c = k_means(data_to_segment[1],9,0.00001,10000)\n# print(cluster)\n# cl = KMeans(n_clusters=9, random_state=0).fit_predict(data_to_segment[1])\n# print(cl)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T09:06:34.300373Z","iopub.execute_input":"2022-04-01T09:06:34.30068Z","iopub.status.idle":"2022-04-01T09:06:34.311869Z","shell.execute_reply.started":"2022-04-01T09:06:34.300645Z","shell.execute_reply":"2022-04-01T09:06:34.311044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Clustering Evaluation","metadata":{}},{"cell_type":"markdown","source":"#### Contingency table calculation","metadata":{}},{"cell_type":"code","source":"def calculate_contingency_matrix(clusters, classes, no_of_clusters, no_of_classes, clusters_labels):\n    contingency_matrix = np.zeros((no_of_clusters, no_of_classes+1)).astype(int)\n    data_length = len(clusters)\n    for pixel in range(data_length):\n        contingency_matrix[clusters_labels.index(clusters[pixel]), classes[pixel]-1] += 1\n    for cluster in range(no_of_clusters):\n        contingency_matrix[cluster, no_of_classes] = sum(contingency_matrix[cluster, :])\n    return contingency_matrix","metadata":{"execution":{"iopub.status.busy":"2022-04-01T09:06:34.313577Z","iopub.execute_input":"2022-04-01T09:06:34.314135Z","iopub.status.idle":"2022-04-01T09:06:34.330528Z","shell.execute_reply.started":"2022-04-01T09:06:34.314092Z","shell.execute_reply":"2022-04-01T09:06:34.329708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Conditional entropy calculation","metadata":{}},{"cell_type":"code","source":"def conditional_entropy(clusters, classes, clusters_labels):\n    no_of_clusters = len(np.unique(clusters))\n    no_of_classes = len(np.unique(classes))\n    no_of_pixels = len(clusters)\n    contingency_matrix = calculate_contingency_matrix(clusters, classes, no_of_clusters, no_of_classes, clusters_labels)\n    conditional_entropy_result = 0\n    for cluster in contingency_matrix:\n        conditional_entropy_given_cluster = 0\n        for a_class in cluster[:no_of_classes]:\n            fraction = a_class / cluster[-1]\n            if fraction > 0:\n                conditional_entropy_given_cluster -= fraction * math.log(fraction, 2)\n        conditional_entropy_result += (cluster[-1] / no_of_pixels) * conditional_entropy_given_cluster\n    return conditional_entropy_result","metadata":{"execution":{"iopub.status.busy":"2022-04-01T09:06:34.332144Z","iopub.execute_input":"2022-04-01T09:06:34.334294Z","iopub.status.idle":"2022-04-01T09:06:34.344984Z","shell.execute_reply.started":"2022-04-01T09:06:34.334233Z","shell.execute_reply":"2022-04-01T09:06:34.344044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### F-Measure calculation","metadata":{}},{"cell_type":"code","source":"def f_measure(clusters, classes, clusters_labels):\n    no_of_clusters = len(np.unique(clusters))\n    no_of_classes = len(np.unique(classes))\n    no_of_pixels = len(clusters)\n    contingency_matrix = calculate_contingency_matrix(clusters, classes, no_of_clusters, no_of_classes, clusters_labels)\n    f_measure_result = 0\n    for cluster in contingency_matrix:\n        max_index = list(cluster[:no_of_classes]).index(max(cluster[:no_of_classes]))\n        purity = cluster[max_index] / cluster[-1]\n        rec = cluster[max_index] / sum(contingency_matrix[:, max_index])\n        f_measure_result += (2 * purity * rec) / (purity + rec)\n    return f_measure_result / no_of_clusters","metadata":{"execution":{"iopub.status.busy":"2022-04-01T09:06:34.349395Z","iopub.execute_input":"2022-04-01T09:06:34.350437Z","iopub.status.idle":"2022-04-01T09:06:34.359745Z","shell.execute_reply.started":"2022-04-01T09:06:34.350382Z","shell.execute_reply":"2022-04-01T09:06:34.35893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_average_measures(cluster, ground_truths, clusters_labels, print_trace = False):\n    average_conditional_entropy = 0\n    average_f_measure = 0\n    for gt_i in range(len(ground_truths)):\n        ce = conditional_entropy(cluster, ground_truths[gt_i], clusters_labels)\n        fm = f_measure(cluster, ground_truths[gt_i], clusters_labels)\n        average_conditional_entropy += ce\n        average_f_measure += fm\n        if print_trace:\n            print('Ground truth #{}: Conditional Entropy = {}, F-Measure = {}'.format(gt_i + 1, ce, fm))\n    average_conditional_entropy /= len(ground_truths)\n    average_f_measure /= len(ground_truths)\n    return average_conditional_entropy, average_f_measure","metadata":{"execution":{"iopub.status.busy":"2022-04-01T09:06:34.361719Z","iopub.execute_input":"2022-04-01T09:06:34.362465Z","iopub.status.idle":"2022-04-01T09:06:34.377301Z","shell.execute_reply.started":"2022-04-01T09:06:34.362413Z","shell.execute_reply":"2022-04-01T09:06:34.376402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Main Code","metadata":{}},{"cell_type":"code","source":"# For each k there is average CE and F_Measure on the whole dataset\ndataset_average_conditional_entropy = [ 0 for k in means]\ndataset_average_f_measure = [ 0 for k in means]\nf_averages = [[] for k in means]\nce_averages = [[] for k in means]\n\nfor (j, s) in enumerate(selected_images):\n    print(\"For image \",j+1)\n    partitions=[]\n    data_to_segment, ground_truths = load_image(s, disp=True)\n    for i in range(len(means)):\n        cluster, c = k_means(data_to_segment,means[i],0.00001,1000)\n        print(\"###################\")\n        print(\"For k = \",means[i])\n        partitions.append(cluster)\n        average_conditional_entropy, average_f_measure = calculate_average_measures(cluster, ground_truths, list(np.unique(cluster)), True)\n        f_averages[i].append((average_f_measure, s)) # lowest f measures = worst\n        ce_averages[i].append((-average_conditional_entropy, s)) # highest ce = worst\n        print('Average Conditional Entropy = {}, Average F-Measure = {}'.format(average_conditional_entropy, average_f_measure))\n        # Division by 50 as selected images number = 50\n        dataset_average_conditional_entropy[i] += average_conditional_entropy / 50\n        dataset_average_f_measure[i] += average_f_measure / 50\n        #print(cluster)\n        \n    display_output(j, partitions, 'kmeans')\n    # Measures\n    print(dataset_average_conditional_entropy)\n    print(dataset_average_f_measure)\n    \n\nfor (i, k) in enumerate(means):\n    f_averages[i].sort()\n    ce_averages[i].sort()\n    print(\"K = {}\".format(k))\n    print(\"Worst by F-measure: \" + ''.join('#{} '.format(s) for _,s in f_averages[i][:3]))\n    print(\"Worst by CE: \"+ ''.join('#{} '.format(s) for _,s in ce_averages[i][:3]))\n    \n    print(\"Best by F-measure: \"+''.join('#{} '.format(s) for _,s in f_averages[i][-3:]))\n    print(\"Best by CE: \"+''.join('#{} '.format(s) for _,s in ce_averages[i][-3:]))","metadata":{"execution":{"iopub.status.busy":"2022-04-01T09:06:34.378526Z","iopub.execute_input":"2022-04-01T09:06:34.379248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Big Picture","metadata":{}},{"cell_type":"code","source":"five_images_sample = [2018, 3063, 5096, 6046, 8068]","metadata":{"execution":{"iopub.status.busy":"2022-04-01T10:33:30.168517Z","iopub.execute_input":"2022-04-01T10:33:30.16896Z","iopub.status.idle":"2022-04-01T10:33:30.178095Z","shell.execute_reply.started":"2022-04-01T10:33:30.168864Z","shell.execute_reply":"2022-04-01T10:33:30.177314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## a. K-means at K = 5 Vs Ground truth","metadata":{}},{"cell_type":"code","source":"for (j, s) in enumerate(five_images_sample):\n    print(\"For image \",j+1)\n    data_to_segment, ground_truths = load_image(s, disp=True)\n    cluster, c = k_means(data_to_segment, 5, 0.00001, 1000)\n    average_conditional_entropy, average_f_measure = calculate_average_measures(cluster, ground_truths, list(np.unique(cluster)))\n    print('Average Conditional Entropy = {}, Average F-Measure = {}'.format(average_conditional_entropy, average_f_measure))\n    display_output(j, [cluster], 'kmeans')","metadata":{"execution":{"iopub.status.busy":"2022-04-01T10:33:32.694857Z","iopub.execute_input":"2022-04-01T10:33:32.696269Z","iopub.status.idle":"2022-04-01T10:34:32.532467Z","shell.execute_reply.started":"2022-04-01T10:33:32.696187Z","shell.execute_reply":"2022-04-01T10:34:32.531115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## b. Normalized-cut at K=5 Vs Ground truth","metadata":{}},{"cell_type":"code","source":"def save_clustering_results(results, name):\n    np.savetxt(name + \".csv\", results, delimiter =\", \", fmt ='% s')","metadata":{"execution":{"iopub.status.busy":"2022-04-01T10:35:40.406313Z","iopub.execute_input":"2022-04-01T10:35:40.406624Z","iopub.status.idle":"2022-04-01T10:35:40.411985Z","shell.execute_reply.started":"2022-04-01T10:35:40.406594Z","shell.execute_reply":"2022-04-01T10:35:40.411039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def apply_normalized_cut(data_to_segment, file_name):\n    affinity = kneighbors_graph(data_to_segment, 5, mode='connectivity')\n    save_clustering_results(spectral_clustering(affinity, n_clusters=5), file_name)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T10:35:41.979873Z","iopub.execute_input":"2022-04-01T10:35:41.981204Z","iopub.status.idle":"2022-04-01T10:35:41.985612Z","shell.execute_reply.started":"2022-04-01T10:35:41.981152Z","shell.execute_reply":"2022-04-01T10:35:41.985049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_clustering_from_file(file_name):\n    with open(file_name, newline='\\n') as file:\n        reader = csv.reader(file)\n        data = [int(row[0]) for row in reader]\n    return data","metadata":{"execution":{"iopub.status.busy":"2022-04-01T10:35:43.634956Z","iopub.execute_input":"2022-04-01T10:35:43.635339Z","iopub.status.idle":"2022-04-01T10:35:43.641218Z","shell.execute_reply.started":"2022-04-01T10:35:43.635298Z","shell.execute_reply":"2022-04-01T10:35:43.640085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for (j, s) in enumerate(five_images_sample):\n    print(\"For image \",j+1)\n    data_to_segment, ground_truths = load_image(s, disp=True)\n    clustering_result_path = '../input/nc-clustering-results/{}_nc_clustering.csv'.format(s)\n    if not os.path.exists(clustering_result_path):\n        apply_normalized_cut(data_to_segment, '{}_nc_clustering'.format(s))\n        clustering_result_path = './{}_nc_clustering.csv'.format(s)\n    cluster = read_clustering_from_file(clustering_result_path)\n    average_conditional_entropy, average_f_measure = calculate_average_measures(cluster, ground_truths, list(np.unique(cluster)))\n    print('Average Conditional Entropy = {}, Average F-Measure = {}'.format(average_conditional_entropy, average_f_measure))\n    display_output(j, [cluster],'normalized_cut')","metadata":{"execution":{"iopub.status.busy":"2022-04-01T10:35:45.135908Z","iopub.execute_input":"2022-04-01T10:35:45.136257Z","iopub.status.idle":"2022-04-01T10:36:37.427384Z","shell.execute_reply.started":"2022-04-01T10:35:45.136216Z","shell.execute_reply":"2022-04-01T10:36:37.426116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## c. Normalized-cut at K=5 Vs K-means at K = 5","metadata":{}},{"cell_type":"code","source":"for (j, s) in enumerate(five_images_sample):\n    print(\"For image \",j+1)\n    data_to_segment, ground_truths = load_image(s, disp=False)\n    clustering_result_path = '../input/nc-clustering-results/{}_nc_clustering.csv'.format(s)\n    if not os.path.exists(clustering_result_path):\n        apply_normalized_cut(data_to_segment, '{}_nc_clustering'.format(s))\n        clustering_result_path = './{}_nc_clustering.csv'.format(s)\n    nc_cluster = read_clustering_from_file(clustering_result_path)\n    k_means_cluster, c = k_means(data_to_segment, 5, 0.00001, 1000)\n    display_output(j, [nc_cluster], 'normalized_cut')\n    display_output(j, [k_means_cluster], 'kmeans')","metadata":{"execution":{"iopub.status.busy":"2022-04-01T10:37:38.327918Z","iopub.execute_input":"2022-04-01T10:37:38.328843Z","iopub.status.idle":"2022-04-01T10:37:58.145161Z","shell.execute_reply.started":"2022-04-01T10:37:38.328768Z","shell.execute_reply":"2022-04-01T10:37:58.144307Z"},"trusted":true},"execution_count":null,"outputs":[]}]}